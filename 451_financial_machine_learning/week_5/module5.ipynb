{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as mpl\n",
    "from sklearn.base import clone\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import cross_val_score\n",
    "import multiprocessing as mp\n",
    "import datetime as dt\n",
    "import sys\n",
    "from sklearn.model_selection._split import _BaseKFold\n",
    "from pathlib import PurePath, Path\n",
    "from itertools import product\n",
    "import time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"#already tranbsformed data from module 4\\nmonthly_final = pd.read_csv('monthly_data.csv')\\nquarterly_final = pd.read_csv('quarterly_data.csv')\\nyearly_final = pd.read_csv('yearly_data.csv')\""
      ]
     },
     "execution_count": 319,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"#already tranbsformed data from module 4\n",
    "monthly_final = pd.read_csv('monthly_data.csv')\n",
    "quarterly_final = pd.read_csv('quarterly_data.csv')\n",
    "yearly_final = pd.read_csv('yearly_data.csv')\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {},
   "outputs": [],
   "source": [
    "def featImpMDI(fit, featNames):\n",
    "    # Mean Decrease Impurity feature importance\n",
    "    df0 = {i:tree.feature_importances_ for i, tree in enumerate(fit.estimators_)}\n",
    "    df0 = pd.DataFrame.from_dict(df0, orient='index')\n",
    "    df0.columns = featNames\n",
    "    df0 = df0.replace(0, np.nan)  # because max_features = 1\n",
    "    imp = pd.concat({'mean': df0.mean(), 'std': df0.std() * df0.shape[0]**-.5}, axis=1)\n",
    "    imp /= imp['mean'].sum()\n",
    "    return imp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getVol(close,span0=100):\n",
    "    # daily vol reindexed to close\n",
    "    df0=close.index.searchsorted(close.index-1)\n",
    "    #bp()\n",
    "    df0=df0[df0>0]\n",
    "    #bp()\n",
    "    df0=(pd.Series(close.index[df0-1],\n",
    "                   index=close.index[close.shape[0]-df0.shape[0]:]))\n",
    "    #bp()\n",
    "    try:\n",
    "        df0=close.loc[df0.index]/close.loc[df0.values].values-1 # daily rets\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        print('adjusting shape of close.loc[df0.index]')\n",
    "        cut = close.loc[df0.index].shape[0] - close.loc[df0.values].shape[0]\n",
    "        df0=close.loc[df0.index].iloc[:-cut]/close.loc[df0.values].values-1\n",
    "    df0=df0.ewm(span=span0).std().rename('dailyVol')\n",
    "    return df0\n",
    "\n",
    "\n",
    "def applyPtSlOnT1(close,events,ptSl,molecule):\n",
    "    # apply stop loss/profit taking, if it takes place before t1 (end of event)\n",
    "    events_=events.loc[molecule]\n",
    "    out=events_[['t1']].copy(deep=True)\n",
    "    if ptSl[0]>0: pt=ptSl[0]*events_['trgt']\n",
    "    else: pt=pd.Series(index=events.index) # NaNs\n",
    "    if ptSl[1]>0: sl=-ptSl[1]*events_['trgt']\n",
    "    else: sl=pd.Series(index=events.index) # NaNs\n",
    "    for loc,t1 in events_['t1'].fillna(close.index[-1]).iteritems():\n",
    "        df0=close[loc:t1] # path prices\n",
    "        df0=(df0/close[loc]-1)*events_.at[loc,'side'] # path returns\n",
    "        out.loc[loc,'sl']=df0[df0<sl[loc]].index.min() # earliest stop loss\n",
    "        out.loc[loc,'pt']=df0[df0>pt[loc]].index.min() # earliest profit taking\n",
    "    return out\n",
    "# =======================================================\n",
    "# Gettting Time of First Touch (getEvents) [3.3]\n",
    "def getEvents(close, tEvents, ptSl, trgt, minRet, numThreads,t1=False, side=None):\n",
    "    #1) get target\n",
    "    trgt=trgt.loc[tEvents]\n",
    "    trgt=trgt[trgt>minRet] # minRet\n",
    "    #2) get t1 (max holding period)\n",
    "    if t1 is False:t1=pd.Series(pd.NaT, index=tEvents)\n",
    "    #3) form events object, apply stop loss on t1\n",
    "    if side is None:side_,ptSl_=pd.Series(1.,index=trgt.index), [ptSl[0],ptSl[0]]\n",
    "    else: side_,ptSl_=side.loc[trgt.index],ptSl[:2]\n",
    "    events=(pd.concat({'t1':t1,'trgt':trgt,'side':side_}, axis=1)\n",
    "            .dropna(subset=['trgt']))\n",
    "    df0=mpPandasObj(func=applyPtSlOnT1,pdObj=('molecule',events.index),\n",
    "                    numThreads=numThreads,close=close,events=events,\n",
    "                    ptSl=ptSl_)\n",
    "    events['t1']=df0.dropna(how='all').min(axis=1) #pd.min ignores nan\n",
    "    if side is None:events=events.drop('side',axis=1)\n",
    "    return events\n",
    "# =======================================================\n",
    "# Adding Vertical Barrier [3.4]\n",
    "def addVerticalBarrier(tEvents, close, numDays=1):\n",
    "    t1=close.index.searchsorted(tEvents+pd.Timedelta(days=numDays))\n",
    "    t1=t1[t1<close.shape[0]]\n",
    "    t1=(pd.Series(close.index[t1],index=tEvents[:t1.shape[0]]))\n",
    "    return t1\n",
    "\n",
    "# =======================================================\n",
    "# Labeling for side and size [3.5, 3.8]\n",
    "\n",
    "\n",
    "def getBins(events, close, t1=None):\n",
    "    '''\n",
    "    Compute event's outcome (including side information, if provided).\n",
    "    events is a DataFrame where:\n",
    "    -events.index is event's starttime\n",
    "    -events['t1'] is event's endtime\n",
    "    -events['trgt'] is event's target\n",
    "    -events['side'] (optional) implies the algo's position side\n",
    "    -t1 is original vertical barrier series\n",
    "    Case 1: ('side' not in events): bin in (-1,1) <-label by price action\n",
    "    Case 2: ('side' in events): bin in (0,1) <-label by pnl (meta-labeling)\n",
    "    '''\n",
    "    # 1) prices aligned with events\n",
    "    events_ = events.dropna(subset=['t1'])\n",
    "    px = events_.index.union(events_['t1'].values).drop_duplicates()\n",
    "    px = close.reindex(px, method='bfill')\n",
    "    # 2) create out object\n",
    "    out = pd.DataFrame(index=events_.index)\n",
    "    out['ret'] = px.loc[events_['t1'].values].values / px.loc[\n",
    "        events_.index] - 1\n",
    "    if 'side' in events_: out['ret'] *= events_['side']  # meta-labeling\n",
    "    out['bin'] = np.sign(out['ret'])\n",
    "\n",
    "    if 'side' not in events_:\n",
    "        # only applies when not meta-labeling.\n",
    "        # to update bin to 0 when vertical barrier is touched, we need the\n",
    "        # original vertical barrier series since the events['t1'] is the time\n",
    "        # of first touch of any barrier and not the vertical barrier\n",
    "        # specifically. The index of the intersection of the vertical barrier\n",
    "        # values and the events['t1'] values indicate which bin labels needs\n",
    "        # to be turned to 0.\n",
    "        vtouch_first_idx = events[events['t1'].isin(t1.values)].index\n",
    "        out.loc[vtouch_first_idx, 'bin'] = 0.\n",
    "\n",
    "    if 'side' in events_: out.loc[out['ret'] <= 0, 'bin'] = 0  # meta-labeling\n",
    "    return out\n",
    "# =======================================================\n",
    "# Expanding getBins to Incorporate Meta-Labeling [3.7]\n",
    "def getBinsOld(events, close):\n",
    "    '''\n",
    "    Compute event's outcome (including side information, if provided).\n",
    "    events is a DataFrame where:\n",
    "    -events.index is event's starttime\n",
    "    -events['t1'] is event's endtime\n",
    "    -events['trgt'] is event's target\n",
    "    -events['side'] (optional) implies the algo's position side\n",
    "    Case 1: ('side' not in events): bin in (-1,1) <-label by price action\n",
    "    Case 2: ('side' in events): bin in (0,1) <-label by pnl (meta-labeling)\n",
    "    '''\n",
    "    #1) prices aligned with events\n",
    "    events_=events.dropna(subset=['t1'])\n",
    "    px=events_.index.union(events_['t1'].values).drop_duplicates()\n",
    "    px=close.reindex(px,method='bfill')\n",
    "    #2) create out object\n",
    "    out=pd.DataFrame(index=events_.index)\n",
    "    out['ret']=px.loc[events_['t1'].values].values/px.loc[events_.index]-1\n",
    "    if 'side' in events_:out['ret']*=events_['side'] # meta-labeling\n",
    "    out['bin']=np.sign(out['ret'])\n",
    "    if 'side' in events_:out.loc[out['ret']<=0,'bin']=0 # meta-labeling\n",
    "    return out\n",
    "# =======================================================\n",
    "# Dropping Unnecessary Labels [3.8]\n",
    "def dropLabels(events, minPct=.05):\n",
    "    # apply weights, drop labels with insufficient examples\n",
    "    while True:\n",
    "        df0=events['bin'].value_counts(normalize=True)\n",
    "        if df0.min()>minPct or df0.shape[0]<3:break\n",
    "        print('dropped label: ', df0.argmin(),df0.min())\n",
    "        events=events[events['bin']!=df0.argmin()]\n",
    "    return events\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PurgedKFold(_BaseKFold):\n",
    "    \"\"\"\n",
    "    Extend KFold class to work with labels that span intervals\n",
    "    The train is purged of observations overlapping test-label intervals\n",
    "    Test set is assumed contiguous (shuffle=False), w/o training samples in between\n",
    "    \"\"\"\n",
    "    def __init__(self,n_splits=3,t1=None,pctEmbargo=0.01):\n",
    "        if not isinstance(t1,pd.Series):\n",
    "            raise ValueError('Label Through Dates must be a pd.Series')\n",
    "        super(PurgedKFold,self).__init__(n_splits,shuffle=False,random_state=None)\n",
    "        self.t1=t1\n",
    "        self.pctEmbargo=pctEmbargo\n",
    "        \n",
    "    def split(self,X,y=None,groups=None):\n",
    "        if (X.index==self.t1.index).sum()!=len(self.t1):\n",
    "            raise ValueError('X and ThruDateValues must have the same index')\n",
    "        indices=np.arange(X.shape[0])\n",
    "        mbrg=int(X.shape[0]*self.pctEmbargo)\n",
    "        test_starts=[\n",
    "            (i[0],i[-1]+1) for i in np.array_split(np.arange(X.shape[0]),\n",
    "                                                   self.n_splits)\n",
    "        ]\n",
    "        for i,j in test_starts:\n",
    "            t0=self.t1.index[i] # start of test set\n",
    "            test_indices=indices[i:j]\n",
    "            maxT1Idx=self.t1.index.searchsorted(self.t1[test_indices].max())\n",
    "            train_indices=self.t1.index.searchsorted(self.t1[self.t1.index>=t0].index)\n",
    "            if maxT1Idx<X.shape[0]: # right train ( with embargo)\n",
    "                train_indices=np.concatenate((train_indices, indices[maxT1Idx+mbrg:]))\n",
    "            yield train_indices,test_indices\n",
    "\n",
    "\n",
    "def featImpMDA(clf,X,y,cv,sample_weight,t1,pctEmbargo,scoring='neg_log_loss'):\n",
    "    # feat imporant based on OOS score reduction\n",
    "    if scoring not in ['neg_log_loss','accuracy']:\n",
    "        raise ValueError('wrong scoring method.')\n",
    "    from sklearn.metrics import log_loss, accuracy_score\n",
    "    cvGen=PurgedKFold(n_splits=cv,t1=t1,pctEmbargo=pctEmbargo) # purged cv\n",
    "    scr0,scr1=pd.Series(), pd.DataFrame(columns=X.columns)\n",
    "\n",
    "    for i,(train,test) in enumerate(cvGen.split(X=X)):\n",
    "        X0,y0,w0=X.iloc[train,:],y.iloc[train],sample_weight.iloc[train]\n",
    "        X1,y1,w1=X.iloc[test,:],y.iloc[test],sample_weight.iloc[test]\n",
    "        fit=clf.fit(X=X0,y=y0,sample_weight=w0.values)\n",
    "        if scoring=='neg_log_loss':\n",
    "            prob=fit.predict_proba(X1)\n",
    "            scr0.loc[i]=-log_loss(y1,prob,sample_weight=w1.values,\n",
    "                                  labels=clf.classes_)\n",
    "        else:\n",
    "            pred=fit.predict(X1)\n",
    "            scr0.loc[i]=accuracy_score(y1,pred,sample_weight=w1.values)\n",
    "\n",
    "    for j in X.columns:\n",
    "        X1_=X1.copy(deep=True)\n",
    "        np.random.shuffle(X1_[j].values) # permutation of a single column\n",
    "        if scoring=='neg_log_loss':\n",
    "            prob=fit.predict_proba(X1_)\n",
    "            scr1.loc[i,j]=-log_loss(y1,prob,sample_weight=w1.values,\n",
    "                                    labels=clf.classes_)\n",
    "        else:\n",
    "            pred=fit.predict(X1_)\n",
    "            scr1.loc[i,j]=accuracy_score(y1,pred,sample_weight=w1.values)\n",
    "    imp=(-scr1).add(scr0,axis=0)\n",
    "    if scoring=='neg_log_loss':imp=imp/-scr1\n",
    "    else: imp=imp/(1.-scr1)\n",
    "    imp=(pd.concat({'mean':imp.mean(),\n",
    "                    'std':imp.std()*imp.shape[0]**-0.5},\n",
    "                   axis=1))\n",
    "    return imp,scr0.mean()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {},
   "outputs": [],
   "source": [
    "def auxFeatImpSFI(featNames, clf, trnsX, cont, scoring, cvGen):\n",
    "    imp = pd.DataFrame(columns=['mean', 'std'])\n",
    "    for featname in featNames:\n",
    "        df0=cvScore(clf, X=trnsX[['featName']], y=cont['bin'], sample_weight=cont['w'], scoring=scoring, cvGen=cvGen)\n",
    "        imp.loc['mean', featname]=df0.mean()\n",
    "        imp.loc['std', featname]=df0.std()*df0.shape[0]**-.5\n",
    "    return imp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_eVec(dot,varThres):\n",
    "    # compute eVec from dot proc matrix, reduce dimension\n",
    "    eVal,eVec=np.linalg.eigh(dot)\n",
    "    idx=eVal.argsort()[::-1] # arugments for sorting eVal desc.\n",
    "    eVal,eVec=eVal[idx],eVec[:,idx]\n",
    "    #2) only positive eVals\n",
    "    eVal=(pd.Series(eVal,index=['PC_'+str(i+1)\n",
    "                                for i in range(eVal.shape[0])]))\n",
    "    eVec=(pd.DataFrame(eVec,index=dot.index,columns=eVal.index))\n",
    "    eVec=eVec.loc[:,eVal.index]\n",
    "    #3) reduce dimension, form PCs\n",
    "    cumVar=eVal.cumsum()/eVal.sum()\n",
    "    dim=cumVar.values.searchsorted(varThres)\n",
    "    eVal,eVec=eVal.iloc[:dim+1],eVec.iloc[:,:dim+1]\n",
    "    return eVal,eVec\n",
    "\n",
    "def orthoFeats(dfx,varThres=0.95):\n",
    "    # given a DataFrame, dfx, of features, compute orthofeatures dfP\n",
    "    dfZ=dfx.sub(dfx.mean(),axis=1).div(dfx.std(),axis=1) # standardize\n",
    "    dot=(pd.DataFrame(np.dot(dfZ.T,dfZ),\n",
    "                      index=dfx.columns,\n",
    "                      columns=dfx.columns))\n",
    "    eVal,eVec=get_eVec(dot,varThres)\n",
    "    dfP=np.dot(dfZ,eVec)\n",
    "    return dfP\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cvScore(clf,X,y,sample_weight,scoring='neg_log_loss',\n",
    "            t1=None,cv=None,cvGen=None,pctEmbargo=.01):\n",
    "    if scoring not in ['neg_log_loss','accuracy']:\n",
    "        raise Exception('wrong scoring method.')\n",
    "    from sklearn.metrics import log_loss,accuracy_score\n",
    "    idx = pd.IndexSlice\n",
    "    if cvGen is None:\n",
    "        cvGen=PurgedKFold(n_splits=cv,t1=t1,pctEmbargo=pctEmbargo) # purged\n",
    "    score=[]\n",
    "    for train,test in cvGen.split(X=X, y=y):\n",
    "        fit=clf.fit(X=X.iloc[idx[train],:],y=y.iloc[idx[train]],\n",
    "                    sample_weight=sample_weight.iloc[idx[train]].values)\n",
    "        if scoring=='neg_log_loss':\n",
    "            prob=fit.predict_proba(X.iloc[idx[test],:])\n",
    "            score_=-log_loss(y.iloc[idx[test]], prob,\n",
    "                             sample_weight=sample_weight.iloc[idx[test]].values,\n",
    "                             labels=clf.classes_)\n",
    "        else:\n",
    "            pred=fit.predict(X.iloc[idx[test],:])\n",
    "            score_=accuracy_score(y.iloc[idx[test]],pred,\n",
    "                                  sample_weight=sample_weight.iloc[idx[test]].values)\n",
    "        score.append(score_)\n",
    "    return np.array(score)\n",
    "\n",
    "def auxFeatImpSFI(featNames, clf, trnsX, cont, scoring, cvGen):\n",
    "    imp = pd.DataFrame(columns=['mean', 'std'])\n",
    "    for featname in featNames:\n",
    "        df0=cvScore(clf, X=trnsX[['featName']], y=cont['target'], sample_weight=cont['w'], scoring=scoring, cvGen=cvGen)\n",
    "        imp.loc['mean', featname]=df0.mean()\n",
    "        imp.loc['std', featname]=df0.std()*df0.shape[0]**-.5\n",
    "    return imp\n",
    "\n",
    "def expandCall(kargs):\n",
    "    # Expand the arguments of a callback function, kargs['func']\n",
    "    func=kargs['func']\n",
    "    del kargs['func']\n",
    "    out=func(**kargs)\n",
    "    return out\n",
    "\n",
    "def reportProgress(jobNum,numJobs,time0,task):\n",
    "    # Report progress as asynch jobs are completed\n",
    "    msg=[float(jobNum)/numJobs, (time.time()-time0)/60.]\n",
    "    msg.append(msg[1]*(1/msg[0]-1))\n",
    "    timeStamp=str(dt.datetime.fromtimestamp(time.time()))\n",
    "    msg=timeStamp+' '+str(round(msg[0]*100,2))+'% '+task+' done after '+ \\\n",
    "        str(round(msg[1],2))+' minutes. Remaining '+str(round(msg[2],2))+' minutes.'\n",
    "    if jobNum<numJobs:sys.stderr.write(msg+'\\r')\n",
    "    else:sys.stderr.write(msg+'\\n')\n",
    "    return\n",
    "\n",
    "def processJobs_(jobs):\n",
    "    # Run jobs sequentially, for debugging\n",
    "    out=[]\n",
    "    for job in jobs:\n",
    "        out_=expandCall(job)\n",
    "        out.append(out_)\n",
    "    return out\n",
    "\n",
    "\n",
    "def processJobs(jobs,task=None,numThreads=24):\n",
    "    # Run in parallel.\n",
    "    # jobs must contain a 'func' callback, for expandCall\n",
    "    if task is None:task=jobs[0]['func'].__name__\n",
    "    pool=mp.Pool(processes=numThreads)\n",
    "    outputs,out,time0=pool.imap_unordered(expandCall,jobs),[],time.time()\n",
    "    # Process asyn output, report progress\n",
    "    for i,out_ in enumerate(outputs,1):\n",
    "        out.append(out_)\n",
    "        reportProgress(i,len(jobs),time0,task)\n",
    "    pool.close();pool.join() # this is needed to prevent memory leaks\n",
    "    return out\n",
    "\n",
    "def linParts(numAtoms,numThreads):\n",
    "    # partition of atoms with a single loop\n",
    "    parts=np.linspace(0,numAtoms,min(numThreads,numAtoms)+1)\n",
    "    parts=np.ceil(parts).astype(int)\n",
    "    return parts\n",
    "\n",
    "def nestedParts(numAtoms,numThreads,upperTriang=False):\n",
    "    # partition of atoms with an inner loop\n",
    "    parts,numThreads_=[0],min(numThreads,numAtoms)\n",
    "    for num in range(numThreads_):\n",
    "        part=1+4*(parts[-1]**2+parts[-1]+numAtoms*(numAtoms+1.)/numThreads_)\n",
    "        part=(-1+part**.5)/2.\n",
    "        parts.append(part)\n",
    "    parts=np.round(parts).astype(int)\n",
    "    if upperTriang: # the first rows are heaviest\n",
    "        parts=np.cumsum(np.diff(parts)[::-1])\n",
    "        parts=np.append(np.array([0]),parts)\n",
    "    return parts\n",
    "\n",
    "def mpPandasObj(func,pdObj,numThreads=24,mpBatches=1,linMols=True,**kargs):\n",
    "    '''\n",
    "    Parallelize jobs, return a dataframe or series\n",
    "    + func: function to be parallelized. Returns a DataFrame\n",
    "    + pdObj[0]: Name of argument used to pass the molecule\n",
    "    + pdObj[1]: List of atoms that will be grouped into molecules\n",
    "    + kwds: any other argument needed by func\n",
    "\n",
    "    Example: df1=mpPandasObj(func,('molecule',df0.index),24,**kwds)\n",
    "    '''\n",
    "    import pandas as pd\n",
    "    #if linMols:parts=linParts(len(argList[1]),numThreads*mpBatches)\n",
    "    #else:parts=nestedParts(len(argList[1]),numThreads*mpBatches)\n",
    "    if linMols:parts=linParts(len(pdObj[1]),numThreads*mpBatches)\n",
    "    else:parts=nestedParts(len(pdObj[1]),numThreads*mpBatches)\n",
    "\n",
    "    jobs=[]\n",
    "    for i in range(1,len(parts)):\n",
    "        job={pdObj[0]:pdObj[1][parts[i-1]:parts[i]],'func':func}\n",
    "        job.update(kargs)\n",
    "        jobs.append(job)\n",
    "    if numThreads==1:out=processJobs_(jobs)\n",
    "    else: out=processJobs(jobs,numThreads=numThreads)\n",
    "    if isinstance(out[0],pd.DataFrame):df0=pd.DataFrame()\n",
    "    elif isinstance(out[0],pd.Series):df0=pd.Series()\n",
    "    else:return out\n",
    "    for i in out:df0=df0.append(i)\n",
    "    df0=df0.sort_index()\n",
    "    return df0\n",
    "\n",
    "def featImportances(trnsX,cont,n_estimators=1000,cv=10,\n",
    "                    max_samples=1.,numThreads=11,pctEmbargo=0.01,\n",
    "                    scoring='accuracy',method='SFI',minWLeaf=0.,**kargs):\n",
    "    # feature importance from a random forest\n",
    "    from sklearn.tree import DecisionTreeClassifier\n",
    "    from sklearn.ensemble import BaggingClassifier\n",
    "    #from mpEngine import mpPandasObj\n",
    "    n_jobs=(-1 if numThreads>1 else 1) # run 1 thread w/ ht_helper in dirac1\n",
    "    #1) prepare classifier,cv. max_features=1, to prevent masking\n",
    "    clf=DecisionTreeClassifier(criterion='entropy',max_features=1,\n",
    "                               class_weight='balanced',\n",
    "                               min_weight_fraction_leaf=minWLeaf)\n",
    "    clf=BaggingClassifier(base_estimator=clf,n_estimators=n_estimators,\n",
    "                          max_features=1.,max_samples=max_samples,\n",
    "                          oob_score=True,n_jobs=n_jobs)\n",
    "    fit=clf.fit(X=trnsX,y=cont['target'],sample_weight=cont['w'].values)\n",
    "    oob=fit.oob_score_\n",
    "    if method=='MDI':\n",
    "        imp=featImpMDI(fit,featNames=trnsX.columns)\n",
    "        oos=cvScore(clf,X=trnsX,y=cont['target'],cv=cv,sample_weight=cont['w'],\n",
    "                    t1=cont['date'],pctEmbargo=pctEmbargo,scoring=scoring).mean()\n",
    "    elif method=='MDA':\n",
    "        imp,oos=featImpMDA(clf,X=trnsX,y=cont['target'],cv=cv,\n",
    "                           sample_weight=cont['w'],t1=cont['date'],\n",
    "                           pctEmbargo=pctEmbargo,scoring=scoring)\n",
    "    elif method=='SFI':\n",
    "        cvGen=PurgedKFold(n_splits=cv,t1=cont['date'],pctEmbargo=pctEmbargo)\n",
    "        oos=cvScore(clf,X=trnsX,y=cont['target'],sample_weight=cont['w'],\n",
    "                    scoring=scoring,cvGen=cvGen).mean()\n",
    "        clf.n_jobs=1 # parallelize auxFeatImpSFI rather than clf\n",
    "        imp=mpPandasObj(auxFeatImpSFI,('featNames',trnsX.columns),numThreads,\n",
    "                        clf=clf,trnsX=trnsX,cont=cont,scoring=scoring,cvGen=cvGen)\n",
    "    return imp,oob,oos\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotFeatImportance(imp,oob,oos,method,pathOut='./',tag=0,simNum=0,**kargs):\n",
    "    # plot mean imp bars with std\n",
    "    mpl.figure(figsize=(10,imp.shape[0]/5.))\n",
    "    imp=imp.sort_values('mean',ascending=True)\n",
    "    ax=imp['mean'].plot(kind='barh',color='b',alpha=0.25,xerr=imp['std'],\n",
    "                        error_kw={'ecolor':'r'})\n",
    "    if method=='MDI':\n",
    "        mpl.xlim([0,imp.sum(axis=1).max()])\n",
    "        mpl.axvline(1./imp.shape[0],lw=1.,color='r',ls='dotted')\n",
    "    ax.get_yaxis().set_visible(False)\n",
    "    for i,j in zip(ax.patches,imp.index):\n",
    "        ax.text(i.get_width()/2, i.get_y()+i.get_height()/2,\n",
    "                j,ha='center',va='center',color='k')\n",
    "    mpl.title('tag='+tag+' | simNUm='+str(simNum)+' | oob='+str(round(oob,4))+' | oos='+str(round(oos,4)))\n",
    "    mpl.savefig(pathOut+'featImportance_'+str(simNum)+'.png',dpi=100)\n",
    "    mpl.clf();mpl.close()\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {},
   "outputs": [],
   "source": [
    "def theFunc(df_,n_estimators=1000,cv=2):\n",
    "    # test the performance of the feat importance functions on artificial data\n",
    "    # Nr noise features = n_featurs-n_informative-n_redundant\n",
    "    trnsX,cont = df_.drop(['target', 'w'], axis=1),df_[['date','target','w']]\n",
    "    dict0={'minWLeaf':[0.],'scoring':['accuracy'],'method':['MDI','MDA','SFI'],\n",
    "           'max_samples':[1.]}\n",
    "    jobs,out=(dict(zip(dict0,i))for i in product(*dict0.values())),[]\n",
    "    kargs={'n_estimators':n_estimators,'tag':'testFunc','cv':cv}\n",
    "    biglist = []\n",
    "    for job in jobs:\n",
    "        #print(job)\n",
    "        smalldict = {}\n",
    "        smalldict['simNum']=job['method']+'_'+job['scoring']+'_'+'%.2f'%job['minWLeaf']+\\\n",
    "            '_'+str(job['max_samples'])\n",
    "        smalldict['method'] = job['method']\n",
    "        smalldict['scoring'] = job['scoring']\n",
    "        smalldict['minWLeaf'] = job['minWLeaf']\n",
    "        smalldict['max_samples'] = job['max_samples']\n",
    "        \n",
    "        job['simNum']=job['method']+'_'+job['scoring']+'_'+'%.2f'%job['minWLeaf']+\\\n",
    "        '_'+str(job['max_samples'])\n",
    "        print(job['simNum'])\n",
    "        kargs.update(job)\n",
    "        imp,oob,oos=featImportances(trnsX=trnsX,cont=cont,**kargs)\n",
    "        \n",
    "        plotFeatImportance(imp=imp,oob=oob,oos=oos,**kargs)\n",
    "        df0=imp[['mean']]/imp['mean'].apply(abs).sum()\n",
    "        #print(df0)\n",
    "        df0['type']=[i[0] for i in df0.index]\n",
    "\n",
    "        #df0=df0.groupby('type')['mean'].apply(abs).sum()\n",
    "        #print(df0)\n",
    "        smalldict['oob']=oob,\n",
    "        smalldict['oos']=oos\n",
    "        biglist.append(smalldict)\n",
    "        #out.append(df0)\n",
    "    #out=(pd.DataFrame(out).sort_values(['method','scoring','minWLeaf','max_samples']))\n",
    "    #out=out['method','scoring','minWLeaf','max_samples','I','R','N','oob','oos']\n",
    "    #out.to_csv('./'+'stats.csv')\n",
    "    return biglist\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'minWLeaf': 0.0, 'scoring': 'accuracy', 'method': 'MDI', 'max_samples': 1.0}\n",
      "MDI_accuracy_0.00_1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\tkkim\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\ensemble\\_base.py:156: FutureWarning: `base_estimator` was renamed to `estimator` in version 1.2 and will be removed in 1.4.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\tkkim\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\ensemble\\_base.py:156: FutureWarning: `base_estimator` was renamed to `estimator` in version 1.2 and will be removed in 1.4.\n",
      "  warnings.warn(\n",
      "c:\\Users\\tkkim\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\ensemble\\_base.py:156: FutureWarning: `base_estimator` was renamed to `estimator` in version 1.2 and will be removed in 1.4.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'minWLeaf': 0.0, 'scoring': 'accuracy', 'method': 'MDA', 'max_samples': 1.0}\n",
      "MDA_accuracy_0.00_1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\tkkim\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\ensemble\\_base.py:156: FutureWarning: `base_estimator` was renamed to `estimator` in version 1.2 and will be removed in 1.4.\n",
      "  warnings.warn(\n",
      "C:\\Users\\tkkim\\AppData\\Local\\Temp\\ipykernel_17132\\42398592.py:39: FutureWarning: The default dtype for empty Series will be 'object' instead of 'float64' in a future version. Specify a dtype explicitly to silence this warning.\n",
      "  scr0,scr1=pd.Series(), pd.DataFrame(columns=X.columns)\n",
      "c:\\Users\\tkkim\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\ensemble\\_base.py:156: FutureWarning: `base_estimator` was renamed to `estimator` in version 1.2 and will be removed in 1.4.\n",
      "  warnings.warn(\n",
      "c:\\Users\\tkkim\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\ensemble\\_base.py:156: FutureWarning: `base_estimator` was renamed to `estimator` in version 1.2 and will be removed in 1.4.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'minWLeaf': 0.0, 'scoring': 'accuracy', 'method': 'SFI', 'max_samples': 1.0}\n",
      "SFI_accuracy_0.00_1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\tkkim\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\ensemble\\_base.py:156: FutureWarning: `base_estimator` was renamed to `estimator` in version 1.2 and will be removed in 1.4.\n",
      "  warnings.warn(\n",
      "c:\\Users\\tkkim\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\ensemble\\_base.py:156: FutureWarning: `base_estimator` was renamed to `estimator` in version 1.2 and will be removed in 1.4.\n",
      "  warnings.warn(\n",
      "c:\\Users\\tkkim\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\ensemble\\_base.py:156: FutureWarning: `base_estimator` was renamed to `estimator` in version 1.2 and will be removed in 1.4.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'time' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [328]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m q \u001b[38;5;241m=\u001b[39m \u001b[43mtheFunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcombined\u001b[49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[1;32mIn [327]\u001b[0m, in \u001b[0;36mtheFunc\u001b[1;34m(df_, n_estimators, cv)\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28mprint\u001b[39m(job[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msimNum\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m     23\u001b[0m kargs\u001b[38;5;241m.\u001b[39mupdate(job)\n\u001b[1;32m---> 24\u001b[0m imp,oob,oos\u001b[38;5;241m=\u001b[39mfeatImportances(trnsX\u001b[38;5;241m=\u001b[39mtrnsX,cont\u001b[38;5;241m=\u001b[39mcont,\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkargs)\n\u001b[0;32m     26\u001b[0m plotFeatImportance(imp\u001b[38;5;241m=\u001b[39mimp,oob\u001b[38;5;241m=\u001b[39moob,oos\u001b[38;5;241m=\u001b[39moos,\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkargs)\n\u001b[0;32m     27\u001b[0m df0\u001b[38;5;241m=\u001b[39mimp[[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmean\u001b[39m\u001b[38;5;124m'\u001b[39m]]\u001b[38;5;241m/\u001b[39mimp[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmean\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28mabs\u001b[39m)\u001b[38;5;241m.\u001b[39msum()\n",
      "Input \u001b[1;32mIn [325]\u001b[0m, in \u001b[0;36mfeatImportances\u001b[1;34m(trnsX, cont, n_estimators, cv, max_samples, numThreads, pctEmbargo, scoring, method, minWLeaf, **kargs)\u001b[0m\n\u001b[0;32m    149\u001b[0m     oos\u001b[38;5;241m=\u001b[39mcvScore(clf,X\u001b[38;5;241m=\u001b[39mtrnsX,y\u001b[38;5;241m=\u001b[39mcont[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtarget\u001b[39m\u001b[38;5;124m'\u001b[39m],sample_weight\u001b[38;5;241m=\u001b[39mcont[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[0;32m    150\u001b[0m                 scoring\u001b[38;5;241m=\u001b[39mscoring,cvGen\u001b[38;5;241m=\u001b[39mcvGen)\u001b[38;5;241m.\u001b[39mmean()\n\u001b[0;32m    151\u001b[0m     clf\u001b[38;5;241m.\u001b[39mn_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m \u001b[38;5;66;03m# parallelize auxFeatImpSFI rather than clf\u001b[39;00m\n\u001b[1;32m--> 152\u001b[0m     imp\u001b[38;5;241m=\u001b[39m\u001b[43mmpPandasObj\u001b[49m\u001b[43m(\u001b[49m\u001b[43mauxFeatImpSFI\u001b[49m\u001b[43m,\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mfeatNames\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mtrnsX\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43mnumThreads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    153\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mclf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclf\u001b[49m\u001b[43m,\u001b[49m\u001b[43mtrnsX\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrnsX\u001b[49m\u001b[43m,\u001b[49m\u001b[43mcont\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcont\u001b[49m\u001b[43m,\u001b[49m\u001b[43mscoring\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mscoring\u001b[49m\u001b[43m,\u001b[49m\u001b[43mcvGen\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcvGen\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    154\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m imp,oob,oos\n",
      "Input \u001b[1;32mIn [325]\u001b[0m, in \u001b[0;36mmpPandasObj\u001b[1;34m(func, pdObj, numThreads, mpBatches, linMols, **kargs)\u001b[0m\n\u001b[0;32m    112\u001b[0m     jobs\u001b[38;5;241m.\u001b[39mappend(job)\n\u001b[0;32m    113\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m numThreads\u001b[38;5;241m==\u001b[39m\u001b[38;5;241m1\u001b[39m:out\u001b[38;5;241m=\u001b[39mprocessJobs_(jobs)\n\u001b[1;32m--> 114\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m: out\u001b[38;5;241m=\u001b[39m\u001b[43mprocessJobs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mjobs\u001b[49m\u001b[43m,\u001b[49m\u001b[43mnumThreads\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnumThreads\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    115\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(out[\u001b[38;5;241m0\u001b[39m],pd\u001b[38;5;241m.\u001b[39mDataFrame):df0\u001b[38;5;241m=\u001b[39mpd\u001b[38;5;241m.\u001b[39mDataFrame()\n\u001b[0;32m    116\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(out[\u001b[38;5;241m0\u001b[39m],pd\u001b[38;5;241m.\u001b[39mSeries):df0\u001b[38;5;241m=\u001b[39mpd\u001b[38;5;241m.\u001b[39mSeries()\n",
      "Input \u001b[1;32mIn [325]\u001b[0m, in \u001b[0;36mprocessJobs\u001b[1;34m(jobs, task, numThreads)\u001b[0m\n\u001b[0;32m     63\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m task \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:task\u001b[38;5;241m=\u001b[39mjobs[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfunc\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\n\u001b[0;32m     64\u001b[0m pool\u001b[38;5;241m=\u001b[39mmp\u001b[38;5;241m.\u001b[39mPool(processes\u001b[38;5;241m=\u001b[39mnumThreads)\n\u001b[1;32m---> 65\u001b[0m outputs,out,time0\u001b[38;5;241m=\u001b[39mpool\u001b[38;5;241m.\u001b[39mimap_unordered(expandCall,jobs),[],\u001b[43mtime\u001b[49m\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m     66\u001b[0m \u001b[38;5;66;03m# Process asyn output, report progress\u001b[39;00m\n\u001b[0;32m     67\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i,out_ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(outputs,\u001b[38;5;241m1\u001b[39m):\n",
      "\u001b[1;31mNameError\u001b[0m: name 'time' is not defined"
     ]
    }
   ],
   "source": [
    "q = theFunc(combined)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setting the random seed to 1, and using the same dataset as in the Module 4 assignment Download the same dataset as in the Module 4 assignment, compute the feature importance scores of each feature by applying the featImportance function (Snippet 8.8 in AFML) on the 10 cross validation sets within the train set defined there.\n",
    "\n",
    " Why canâ€™t FS be applied to the train set as a whole? (For a hint, please read assetCode with MDA using random dataLinks to an external site..) (16 points)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xdat = pd.read_csv('X_processed_final.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ydat = pd.read_csv('y_final.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined = pd.concat([xdat, ydat], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xdat = pd.read_csv('X_processed_final.csv')\n",
    "ydat = pd.read_csv('y_final.csv')\n",
    "combined = pd.concat([xdat, ydat], axis=1)\n",
    "combined.rename(columns={'ret':'target', 'yyyymm':'date'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>yyyymm</th>\n",
       "      <th>ntis</th>\n",
       "      <th>infl</th>\n",
       "      <th>ltr</th>\n",
       "      <th>corpr</th>\n",
       "      <th>svar</th>\n",
       "      <th>d/p</th>\n",
       "      <th>d/y</th>\n",
       "      <th>e/p</th>\n",
       "      <th>d/e</th>\n",
       "      <th>tms</th>\n",
       "      <th>dfy</th>\n",
       "      <th>dfr</th>\n",
       "      <th>b/m_FD</th>\n",
       "      <th>tbl_FD</th>\n",
       "      <th>lty_FD</th>\n",
       "      <th>ret</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1945-12-01</td>\n",
       "      <td>0.025525</td>\n",
       "      <td>0.005525</td>\n",
       "      <td>0.0194</td>\n",
       "      <td>0.0133</td>\n",
       "      <td>0.001538</td>\n",
       "      <td>-3.269684</td>\n",
       "      <td>-3.337063</td>\n",
       "      <td>-2.894991</td>\n",
       "      <td>-0.374693</td>\n",
       "      <td>0.0161</td>\n",
       "      <td>0.0049</td>\n",
       "      <td>-0.0061</td>\n",
       "      <td>-0.216009</td>\n",
       "      <td>-2.99635</td>\n",
       "      <td>-1.507835</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1946-01-01</td>\n",
       "      <td>0.027969</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0025</td>\n",
       "      <td>0.0128</td>\n",
       "      <td>0.002645</td>\n",
       "      <td>-3.327012</td>\n",
       "      <td>-3.255014</td>\n",
       "      <td>-2.983423</td>\n",
       "      <td>-0.343589</td>\n",
       "      <td>0.0161</td>\n",
       "      <td>0.0047</td>\n",
       "      <td>0.0103</td>\n",
       "      <td>-0.264780</td>\n",
       "      <td>-2.99635</td>\n",
       "      <td>-1.486065</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1946-02-01</td>\n",
       "      <td>0.031091</td>\n",
       "      <td>-0.005495</td>\n",
       "      <td>0.0032</td>\n",
       "      <td>0.0034</td>\n",
       "      <td>0.005942</td>\n",
       "      <td>-3.245065</td>\n",
       "      <td>-3.290322</td>\n",
       "      <td>-2.932931</td>\n",
       "      <td>-0.312134</td>\n",
       "      <td>0.0160</td>\n",
       "      <td>0.0047</td>\n",
       "      <td>0.0002</td>\n",
       "      <td>-0.167204</td>\n",
       "      <td>-2.99635</td>\n",
       "      <td>-1.481915</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1946-03-01</td>\n",
       "      <td>0.031188</td>\n",
       "      <td>0.011050</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>0.0034</td>\n",
       "      <td>0.001497</td>\n",
       "      <td>-3.280469</td>\n",
       "      <td>-3.317389</td>\n",
       "      <td>-3.000167</td>\n",
       "      <td>-0.280302</td>\n",
       "      <td>0.0160</td>\n",
       "      <td>0.0047</td>\n",
       "      <td>0.0024</td>\n",
       "      <td>-0.190201</td>\n",
       "      <td>-2.99635</td>\n",
       "      <td>-1.474585</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1946-04-01</td>\n",
       "      <td>0.031307</td>\n",
       "      <td>0.005464</td>\n",
       "      <td>-0.0135</td>\n",
       "      <td>-0.0043</td>\n",
       "      <td>0.001078</td>\n",
       "      <td>-3.317389</td>\n",
       "      <td>-3.339531</td>\n",
       "      <td>-3.059560</td>\n",
       "      <td>-0.257829</td>\n",
       "      <td>0.0169</td>\n",
       "      <td>0.0050</td>\n",
       "      <td>0.0092</td>\n",
       "      <td>-0.222940</td>\n",
       "      <td>-2.99635</td>\n",
       "      <td>-1.425691</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       yyyymm      ntis      infl     ltr   corpr      svar       d/p  \\\n",
       "0  1945-12-01  0.025525  0.005525  0.0194  0.0133  0.001538 -3.269684   \n",
       "1  1946-01-01  0.027969  0.000000  0.0025  0.0128  0.002645 -3.327012   \n",
       "2  1946-02-01  0.031091 -0.005495  0.0032  0.0034  0.005942 -3.245065   \n",
       "3  1946-03-01  0.031188  0.011050  0.0010  0.0034  0.001497 -3.280469   \n",
       "4  1946-04-01  0.031307  0.005464 -0.0135 -0.0043  0.001078 -3.317389   \n",
       "\n",
       "        d/y       e/p       d/e     tms     dfy     dfr    b/m_FD   tbl_FD  \\\n",
       "0 -3.337063 -2.894991 -0.374693  0.0161  0.0049 -0.0061 -0.216009 -2.99635   \n",
       "1 -3.255014 -2.983423 -0.343589  0.0161  0.0047  0.0103 -0.264780 -2.99635   \n",
       "2 -3.290322 -2.932931 -0.312134  0.0160  0.0047  0.0002 -0.167204 -2.99635   \n",
       "3 -3.317389 -3.000167 -0.280302  0.0160  0.0047  0.0024 -0.190201 -2.99635   \n",
       "4 -3.339531 -3.059560 -0.257829  0.0169  0.0050  0.0092 -0.222940 -2.99635   \n",
       "\n",
       "     lty_FD  ret  \n",
       "0 -1.507835  1.0  \n",
       "1 -1.486065 -1.0  \n",
       "2 -1.481915  1.0  \n",
       "3 -1.474585  1.0  \n",
       "4 -1.425691  1.0  "
      ]
     },
     "execution_count": 194,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined.rename(columns={'ret':'target', 'yyyymm':'date'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       " 1.0    499\n",
       "-1.0    378\n",
       "Name: target, dtype: int64"
      ]
     },
     "execution_count": 196,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined['target'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "buy_weight = combined[combined['target'] == 1].shape[0] / combined.shape[0]\n",
    "sell_weight = combined[combined['target'] == -1].shape[0] / combined.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined['w'] = np.where(combined['target'] == 1, buy_weight, sell_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined['date'] = (combined['date'].astype(str).str.split('-').str[0] + combined['date'].astype(str).str.split('-').str[1]).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combinedx = combined.drop(['target', 'w'], axis=1)\n",
    "combinedy = combined[['target', 'w']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'minWLeaf': 0.0, 'scoring': 'accuracy', 'method': 'MDI', 'max_samples': 1.0}\n",
      "MDI_accuracy_0.00_1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\tkkim\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\ensemble\\_base.py:156: FutureWarning: `base_estimator` was renamed to `estimator` in version 1.2 and will be removed in 1.4.\n",
      "  warnings.warn(\n",
      "c:\\Users\\tkkim\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\ensemble\\_base.py:156: FutureWarning: `base_estimator` was renamed to `estimator` in version 1.2 and will be removed in 1.4.\n",
      "  warnings.warn(\n",
      "c:\\Users\\tkkim\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\ensemble\\_base.py:156: FutureWarning: `base_estimator` was renamed to `estimator` in version 1.2 and will be removed in 1.4.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'minWLeaf': 0.0, 'scoring': 'accuracy', 'method': 'MDA', 'max_samples': 1.0}\n",
      "MDA_accuracy_0.00_1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\tkkim\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\ensemble\\_base.py:156: FutureWarning: `base_estimator` was renamed to `estimator` in version 1.2 and will be removed in 1.4.\n",
      "  warnings.warn(\n",
      "C:\\Users\\tkkim\\AppData\\Local\\Temp\\ipykernel_17132\\42398592.py:39: FutureWarning: The default dtype for empty Series will be 'object' instead of 'float64' in a future version. Specify a dtype explicitly to silence this warning.\n",
      "  scr0,scr1=pd.Series(), pd.DataFrame(columns=X.columns)\n",
      "c:\\Users\\tkkim\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\ensemble\\_base.py:156: FutureWarning: `base_estimator` was renamed to `estimator` in version 1.2 and will be removed in 1.4.\n",
      "  warnings.warn(\n",
      "c:\\Users\\tkkim\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\ensemble\\_base.py:156: FutureWarning: `base_estimator` was renamed to `estimator` in version 1.2 and will be removed in 1.4.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'minWLeaf': 0.0, 'scoring': 'accuracy', 'method': 'SFI', 'max_samples': 1.0}\n",
      "SFI_accuracy_0.00_1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\tkkim\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\ensemble\\_base.py:156: FutureWarning: `base_estimator` was renamed to `estimator` in version 1.2 and will be removed in 1.4.\n",
      "  warnings.warn(\n",
      "c:\\Users\\tkkim\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\ensemble\\_base.py:156: FutureWarning: `base_estimator` was renamed to `estimator` in version 1.2 and will be removed in 1.4.\n",
      "  warnings.warn(\n",
      "c:\\Users\\tkkim\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\ensemble\\_base.py:156: FutureWarning: `base_estimator` was renamed to `estimator` in version 1.2 and will be removed in 1.4.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "q = theFunc(combined)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
